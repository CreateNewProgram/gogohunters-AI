from fastapi import FastAPI, WebSocket
import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
from gtts import gTTS
from pydub import AudioSegment
import numpy as np
import io
import os
from dotenv import load_dotenv
from google.generativeai import GenerativeModel, configure

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Gemini ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
configure(api_key=GEMINI_API_KEY)
gemini = GenerativeModel("models/gemini-1.5-pro")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Whisper STT ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸŸ¡ Whisper ì´ˆê¸°í™” ì‹œì‘")

device = "cuda:0" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

model_id = "openai/whisper-small"

print(f"ğŸŸ¡ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_id} (device={device})")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
)
model.to(device)
print("ğŸŸ¢ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

print("ğŸŸ¡ í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
processor = AutoProcessor.from_pretrained(model_id)
print("ğŸŸ¢ í”„ë¡œì„¸ì„œ ë¡œë“œ ì™„ë£Œ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WebSocket ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("âœ… í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨")

    try:
        # 1íšŒë§Œ ì²˜ë¦¬
        print("ğŸŸ¡ JSON ë©”íƒ€ë°ì´í„° ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
        json_meta = await websocket.receive_json()
        print("âœ… ë©”íƒ€ë°ì´í„° ìˆ˜ì‹ :", json_meta)

        print("ğŸŸ¡ ìŒì„± ë°”ì´ë„ˆë¦¬ ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
        audio_binary = await websocket.receive_bytes()
        print(f"âœ… ì˜¤ë””ì˜¤ {len(audio_binary)} bytes ìˆ˜ì‹  ì™„ë£Œ")

        # pydub ë¡œ wav/pcm ë””ì½”ë”©
        audio = AudioSegment.from_file(io.BytesIO(audio_binary))
        audio = audio.set_frame_rate(16000).set_channels(1)  # Whisper ê¶Œì¥ 16kHz mono
        samples = np.array(audio.get_array_of_samples()).astype(np.float32) / 32768.0

        # STT
        print("ğŸŸ¡ STT ë³€í™˜ ì‹œì‘...")
        inputs = processor(samples, sampling_rate=16000, return_tensors="pt").to(device)
        forced_decoder_ids = processor.get_decoder_prompt_ids(language="ko", task="transcribe")

        with torch.no_grad():
            generated_ids = model.generate(
                inputs.input_features,
                forced_decoder_ids=forced_decoder_ids
            )
        transcribed_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        print(f"ğŸ“ STT ê²°ê³¼: {transcribed_text}")

        # ì±—ë´‡(Gemini)
        print("ğŸŸ¡ Gemini ì‘ë‹µ ìƒì„± ì¤‘...")
        chat = gemini.start_chat(history=[])
        prompt = f"'{transcribed_text}' ë¼ëŠ” ë°œí™”ì— ëŒ€í•´ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜."
        gemini_response = chat.send_message(prompt)
        answer_text = gemini_response.text
        print(f"ğŸ“ Gemini ì‘ë‹µ: {answer_text}")

        # TTS
        print("ğŸŸ¡ TTS ë³€í™˜(gTTS) ì‹œì‘...")
        tts = gTTS(text=answer_text, lang="ko")
        tts.save("output.mp3")
        print("ğŸŸ¢ TTS ë³€í™˜ ì™„ë£Œ, mp3 ìƒì„±ë¨")

        print("ğŸŸ¡ mp3 â†’ PCM ë³€í™˜ ì‹œì‘...")
        sound = AudioSegment.from_mp3("output.mp3")
        pcm_io = io.BytesIO()
        sound.export(pcm_io, format="wav", parameters=["-acodec", "pcm_s16le"])
        pcm_io.seek(0)
        print("ğŸŸ¢ PCM ë³€í™˜ ì™„ë£Œ")

        response_meta = {
            "user_id": json_meta["user_id"],
            "sequence": json_meta["sequence"],
            "timestamp": json_meta["timestamp"],
            "answer": answer_text
        }
        print("ğŸŸ¡ ì‘ë‹µ JSON ì „ì†¡ ì¤‘...")
        await websocket.send_json(response_meta)
        print("ğŸŸ¡ ìŒì„± ë°ì´í„°(PCM) ì „ì†¡ ì¤‘...")
        await websocket.send_bytes(pcm_io.read())

        print("âœ… ì‘ë‹µ ì „ì†¡ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
    finally:
        print("ğŸ”´ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ")
        await websocket.close()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ ì„œë²„ ì‹¤í–‰ ì¤€ë¹„ì™„ë£Œ")
    uvicorn.run("chat_server_gemini:app", host="127.0.0.1", port=8090, reload=True)

#  cloudflared tunnel --url http://localhost:8090