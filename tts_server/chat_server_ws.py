from fastapi import FastAPI, WebSocket
import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
from gtts import gTTS
from pydub import AudioSegment
import io

app = FastAPI()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Whisper STT ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸŸ¡ Whisper ì´ˆê¸°í™” ì‹œì‘")

device = "cuda:0" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

model_id = "openai/whisper-small"

print(f"ğŸŸ¡ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_id} (device={device})")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
)
model.to(device)
print("ğŸŸ¢ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

print("ğŸŸ¡ í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
processor = AutoProcessor.from_pretrained(model_id)
print("ğŸŸ¢ í”„ë¡œì„¸ì„œ ë¡œë“œ ì™„ë£Œ")

print("ğŸŸ¡ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    max_new_tokens=128,
    chunk_length_s=30,
    batch_size=16,
    return_timestamps=True,
    torch_dtype=torch_dtype,
    device=device,
)
print("ğŸŸ¢ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WebSocket ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("âœ… í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨")

    try:
        # 1íšŒë§Œ ì²˜ë¦¬
        print("ğŸŸ¡ JSON ë©”íƒ€ë°ì´í„° ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
        json_meta = await websocket.receive_json()
        print("âœ… ë©”íƒ€ë°ì´í„° ìˆ˜ì‹ :", json_meta)

        print("ğŸŸ¡ ìŒì„± ë°”ì´ë„ˆë¦¬ ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
        audio_binary = await websocket.receive_bytes()
        print(f"âœ… ì˜¤ë””ì˜¤ {len(audio_binary)} bytes ìˆ˜ì‹  ì™„ë£Œ")

        # STT
        print("ğŸŸ¡ STT ë³€í™˜ ì‹œì‘...")
        result = pipe(audio_binary)
        transcribed_text = result["text"]
        print(f"ğŸ“ STT ê²°ê³¼: {transcribed_text}")

        # ì±—ë´‡
        print("ğŸŸ¡ ì±—ë´‡ ì‘ë‹µ ìƒì„± ì¤‘...")
        answer_text = f"ë„ˆê°€ ë§í•œ '{transcribed_text}' ì˜ ì˜ë¯¸ë¥¼ ì„¤ëª…í•´ì¤„ê²Œ."
        print(f"ğŸ“ ì±—ë´‡ ì‘ë‹µ: {answer_text}")

        # TTS
        print("ğŸŸ¡ TTS ë³€í™˜(gTTS) ì‹œì‘...")
        tts = gTTS(text=answer_text, lang="ko")
        tts.save("output.mp3")
        print("ğŸŸ¢ TTS ë³€í™˜ ì™„ë£Œ, mp3 ìƒì„±ë¨")

        print("ğŸŸ¡ mp3 â†’ PCM ë³€í™˜ ì‹œì‘...")
        sound = AudioSegment.from_mp3("output.mp3")
        pcm_io = io.BytesIO()
        sound.export(pcm_io, format="wav", parameters=["-acodec", "pcm_s16le"])
        pcm_io.seek(0)
        print("ğŸŸ¢ PCM ë³€í™˜ ì™„ë£Œ")

        response_meta = {
            "user_id": json_meta["user_id"],
            "sequence": json_meta["sequence"],
            "timestamp": json_meta["timestamp"],
            "answer": answer_text
        }
        print("ğŸŸ¡ ì‘ë‹µ JSON ì „ì†¡ ì¤‘...")
        await websocket.send_json(response_meta)
        print("ğŸŸ¡ ìŒì„± ë°ì´í„°(PCM) ì „ì†¡ ì¤‘...")
        await websocket.send_bytes(pcm_io.read())

        print("âœ… ì‘ë‹µ ì „ì†¡ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
    finally:
        print("ğŸ”´ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ")
        await websocket.close()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ ì„œë²„ ì‹¤í–‰ ì¤€ë¹„ì™„ë£Œ")
    uvicorn.run("chat_server_ws:app", host="127.0.0.1", port=8090, reload=True)
